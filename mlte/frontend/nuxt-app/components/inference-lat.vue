<template>
  <div>
    <br />
    <!-- API call -->
    <div class="chatgptcall">
      <p><b>What's Inference Latency?</b> </p>
      <p>
        Inference latency refers to the time taken for a trained model to process an input and generate a prediction. 
        <span class="AIgeneratedtext">{{ response }} </span>
      </p>
    </div>
    <br />
    <!-- <p> <b>What does that mean for your project?</b></p> -->
    <p><b>Create a Quality Scenario</b></p>
    <br/>

     
<div>
    <!-- Deployment Option -->
    <label><b>Inference Option</b></label>
    <div class="info-container">
      <span class="info-icon">i</span>
      <div class="tooltip">edit this...</div>
    </div>
    <USelect
      placeholder="Select an option..."
      :options="['Batch Inference (the model makes predictions on a bunch of common unlabeled examples and then caches those prediction)', 'Streaming Inference (model only makes predictions on demand)', 'To Be Defined','Other']"
      icon="i-heroicons-magnifying-glass-20-solid"
      v-model="deploymentInfrastructure"
      style="width: 300px;" 
    />
    <br />

    <!-- Deployment Infrastructure -->
    <label><b>Deployment Infrastructure</b></label>
    <div class="info-container">
      <span class="info-icon">i</span>
      <div class="tooltip">Select where your model will be deployed</div>
    </div>
    <USelect
      placeholder="Select an option..."
      :options="['Cloud (enables scalable, remote access to computational resources and storage for inference)', 'On-premise (model runs on local, in-house servers, offering full control over the environment but requiring in-house maintenance)', 'Edge (allow real-time predictions and low-latency while minimizing the need for data transmission to centralized servers)', 'To Be Defined', 'Other']"
      icon="i-heroicons-magnifying-glass-20-solid"
      v-model="infrastructureDetails"
      style="width: 300px;" 
    />
    <br /> 

    <!-- Inference Latency Metrics -->
    <label><b>Average Expected Latency in Seconds</b></label>
    <div class="info-container">
      <span class="info-icon">i</span>
      <div class="tooltip">Enter the average time (in seconds) that you expect for inference to complete under normal conditions.</div>
    </div>
    <UInput 
    v-model="averageLatency" 
    placeholder="X seconds" 
    style="width: 300px;" 
     />
    <br />

    <label><b>Percentage of Requests to Meet Target</b></label>
    <div class="info-container">
      <span class="info-icon">i</span>
      <div class="tooltip">Specify the latency (in seconds) that 90% of your requests should meet.</div>
    </div>
    <UInput v-model="mostLatency" placeholder="90%" style="width: 300px;"  />
    <br />

    <label><b>Latency in Seconds</b></label>
    <div class="info-container">
      <span class="info-icon">i</span>
      <div class="tooltip">Maximum acceptable latency time (in seconds)</div>
    </div>
    <UInput 
    v-model="latencySeconds" 
    placeholder="X seconds"
    style="width: 300px;"  />
    <br />

      <!-- Dynamic Sentence -->
      <p class="input-group" style="padding-top: 10px; padding-bottom: 10px">
  <b>Scenario for Inference Latency:</b> Model's inference is [{{ firstWordOfDeploymentInfrastructure }}], with an average latency of [{{ averageLatency }}]. [{{ mostLatency }} ]of requests will have a maximum latency of [{{ latencySeconds }}]. The model's deployment infrastructure is [{{firstWordOfinfrastructureDetails }}].
</p>
<br/>
<UButton color="yellow" :ui="{ rounded: 'rounded-full' }" @click="checkMetrics">Do these metrics make sense?</UButton>

<br/>
<br/>

<!-- Display the 2nd call -->
 <p v-if="secondResponse">
  <span class ="AIgeneratedtext">{{ secondResponse }}</span>
</p>

<br/>
<span class="AIgeneratedtext" id="cautiontext"> Highlighted text has been generated by AI </span>
</div>
</div>

<!-- -->

<p> </p>

</template>

<script lang="ts">
import { ref, onMounted, computed } from 'vue';
import { openai } from '~/composables/openai';

export default {
  name: 'InferenceLatencyForm',
  props: {
    MLTask: {
      required: true,
    },
    usageContext: {
      required: true,
    },
  },
  setup(props) {
    const response = ref('');
    const secondResponse = ref('');
    const deploymentInfrastructure = ref<string | null>(null);
    const infrastructureDetails = ref<string | null>(null);
    const averageLatency = ref<string | null>(null);
    const mostLatency = ref<string | null>(null);
    const latencySeconds = ref<string | null>(null);

    //  first word of deploymentInfrastructure and detailsa
    const firstWordOfDeploymentInfrastructure = computed(() => {
      if (deploymentInfrastructure.value) {
        return deploymentInfrastructure.value.split(' ')[0]; 
      }
      return '';
    });

    const firstWordOfinfrastructureDetails = computed(() => {
      if (infrastructureDetails.value) {
        return infrastructureDetails.value.split(' ')[0]; 
      }
      return '';
    });

    const chat_role = 'You are a specialized data scientist with knowledge in both software engineering and data science.';

    const getChatResponse = async () => {
      const { chat } = openai();
      try {
        const messages = [
          {
            role: 'system',
            content: chat_role,
          },
          {
            role: 'user',
            content: `Write one sentence to explain the potential consequences of not considering inference latency in the context of ${props.MLTask} and ${props.usageContext}. Use simple language that data scientists would understand`,
          },
        ];

        const chatResponse = await chat(messages, 'gpt-3.5-turbo');
        const splitResponse = chatResponse.split('\n\n');
        response.value = splitResponse[0];
      } catch (error) {
        console.error('Error fetching chat response:', error);
      }
    };


    // Second API call on button click
    const checkMetrics = async () => {
      const { chat } = openai();
      try {
        const messages = [
          {
            role: 'system',
            content: chat_role,
          },
          {
            role: 'user',
            content: `Please review the following latency metrics for ${props.MLTask}:
            - Average Latency: ${averageLatency.value} seconds
            - Most Latency: ${mostLatency.value}% requests should meet this latency
            - Latency in Seconds: ${latencySeconds.value} seconds

            Do these metrics make sense? What would you suggest?`,
          },
        ];

        const chatResponse = await chat(messages, 'gpt-3.5-turbo');
        const splitResponse = chatResponse.split('\n\n');
        secondResponse.value = splitResponse;  
      } catch (error) {
        console.error('Error fetching metrics response:', error);
      }
    };


    onMounted(() => {
      getChatResponse(); // initial call 
    });

    return {
      response,
      secondResponse,
      deploymentInfrastructure,
      infrastructureDetails,
      averageLatency,
      mostLatency,
      latencySeconds,
      firstWordOfDeploymentInfrastructure, 
      firstWordOfinfrastructureDetails,
      checkMetrics,
    };
  },
};
</script>


<style scoped>
.AIgeneratedtext{
  background-color: #efe8c7;
}

.info-icon {
  display: inline-block;
  width: 20px;
  height: 20px;
  background-color: black;
  color: white;
  border-radius: 50%;
  text-align: center;
  line-height: 20px;
  font-weight: bold;
  font-family: Arial, cursive;
  font-size: 10px;
  cursor: pointer;
  margin-left: 5px;
  position: relative;
}

.tooltip {
  display: none;
  position: absolute;
  background-color: rgb(0, 0, 0);
  color: rgb(255, 255, 255);
  border: 1px solid #ccc;
  padding: 10px;
  font-size: 12px;
  border-radius: 5px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
  width: 200px;
  top: 25px; 
  left: 0;
  z-index: 10;
}

.info-container:hover .tooltip {
  display: block;
}

.info-container {
  position: relative;
  display: inline-block;
}

#cautiontext{
  font-size: 12px;
  font-style: italic;
  text-align: center;
}
</style>